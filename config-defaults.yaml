---
    run_name:
        description: wandb run name
        value: 'test-run-1'

    output_dir:
        description: output directory for saved models
        value: './models'

    # we don't use epochs because training between models is only comparable with steps and batch size
    # whereas epochs is dataset dependent
    batch_size:
        description: training batch size (per gpu device)
        value: 128

    total_steps:
        description: number of training steps
        value: 500.0e+3

    save_steps:
        description: intervals before model is saved
        value: 5.0e+3

    logging_steps:
        description: intervals before wandb logging
        value: 50

    mlm_probability:
        description: probability of mlm masking being applied to a train sequence token
        value: 0.15

    # optimization parameters
    learning_rate:
        description: The initial weight decay for the AdamW optimizer
        value: 3.0e-4

    weight_decay:
        description: The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in AdamW optimizer.
        value: 0.01

    adam_beta1:
        description: The beta1 hyperparameter for the AdamW optimizer
        value: 0.9

    adam_beta2:
        description: The beta2 hyperparameter for the AdamW optimizer
        value: 0.999

    adam_epsilon:
        description: The epsilon hyperparameter for the AdamW optimizer
        value: 1.0e-6

    hidden_dropout:
        description: Dropout for hidden layers
        value: 0.1

    attention_dropout:
        description: Dropout for attention
        value: 0.1