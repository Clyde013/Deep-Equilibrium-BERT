---
    run_name:
        description: wandb run name
        value: 'lucas-config-cont'

    output_dir:
        description: output directory for saved models
        value: './models/lucas-cont/'

    load_checkpoint:
        description: load the model from this checkpoint (use null if no checkpoint)
        value: './models/base/checkpoint-50000/'

    resume_steps:
        description: resuming training from checkpoint at this number of steps (if not resuming set to -1)
        value: 60000

    # we don't use epochs because training between models is only comparable with steps and batch size
    # whereas epochs is dataset dependent
    batch_size:
        description: training batch size (per gpu device)
        value: 64

    # effective batch size is batch_size * grad_accum_steps
    grad_accum_steps:
        descriptions: number of steps to accumulate gradient for before performing backward pass
        value: 2

    total_steps:
        description: number of training steps
        value: 500.0e+3

    save_steps:
        description: intervals before model is saved
        value: 5.0e+3

    logging_steps:
        description: intervals before wandb logging
        value: 50

    mlm_probability:
        description: probability of mlm masking being applied to a train sequence token
        value: 0.15

    # optimization parameters
    learning_rate:
        description: The initial weight decay for the AdamW optimizer
        value: 1.2e-4

    weight_decay:
        description: The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in AdamW optimizer.
        value: 0.01

    adam_beta1:
        description: The beta1 hyperparameter for the AdamW optimizer
        value: 0.9

    adam_beta2:
        description: The beta2 hyperparameter for the AdamW optimizer
        value: 0.999

    adam_epsilon:
        description: The epsilon hyperparameter for the AdamW optimizer
        value: 1.0e-6

    hidden_dropout:
        description: Dropout for hidden layers
        value: 0.1

    attention_dropout:
        description: Dropout for attention
        value: 0.1