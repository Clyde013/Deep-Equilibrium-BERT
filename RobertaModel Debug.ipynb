{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6db5df78-ec48-47d7-ab8b-7133cdf8aa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from roberta import RobertaModel\n",
    "from transformers import RobertaTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "152933a4-3bc7-4fef-8054-f537377c9816",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "#inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "inputs = {'input_ids': torch.tensor([[    0, 31414,     6,   127,  2335,    16, 11962,     2]]), 'attention_mask': torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55c38d06-26d4-493b-b5dc-0cdbb13c0744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaLayer forward inputs hidden_states: ftorch.Size([1, 8, 768])\n",
      "RobertaLayer forward inputs attention_mask: ftorch.Size([1, 1, 1, 8])\n",
      "RobertaLayer forward outputs: f(tensor([[[-4.5309e-02,  1.2841e-02,  6.1168e-02,  ...,  1.2439e-04,\n",
      "          -1.7222e-02, -1.0805e-01],\n",
      "         [-4.9035e-01, -8.0960e-01,  2.0653e-01,  ..., -2.1987e-01,\n",
      "          -3.9566e-01,  5.2398e-01],\n",
      "         [ 2.2331e-01, -7.4813e-02, -7.0698e-02,  ..., -1.1411e+00,\n",
      "          -1.1119e-01,  4.3990e-01],\n",
      "         ...,\n",
      "         [ 2.6296e-01, -1.9019e-01,  3.6889e-01,  ...,  4.9382e-02,\n",
      "          -2.2182e-01,  6.7661e-02],\n",
      "         [-3.7124e-01,  2.3186e-01,  4.2385e-01,  ...,  5.9505e-01,\n",
      "          -1.0074e+00,  3.5123e-03],\n",
      "         [ 7.6510e-02, -1.5070e-01,  4.5166e-01,  ...,  5.4768e-01,\n",
      "           5.7989e-02, -5.7921e-01]]], grad_fn=<NativeLayerNormBackward0>),)\n",
      "RobertaLayer forward inputs hidden_states: ftorch.Size([1, 8, 768])\n",
      "RobertaLayer forward inputs attention_mask: ftorch.Size([1, 1, 1, 8])\n",
      "RobertaLayer forward outputs: f(tensor([[[ 0.0498,  0.0261, -0.0118,  ...,  0.0277, -0.0195, -0.0542],\n",
      "         [-0.5419, -0.5913, -0.1627,  ...,  0.1122, -0.3579,  0.3498],\n",
      "         [ 0.4223,  0.1942, -0.1123,  ..., -0.8358, -0.3345,  0.6133],\n",
      "         ...,\n",
      "         [-0.1317,  0.2040,  0.2335,  ...,  0.0867, -0.4717,  0.2111],\n",
      "         [-0.6625,  0.4531,  0.1510,  ...,  0.8153, -1.1790,  0.0881],\n",
      "         [-0.1281,  0.0462,  0.0563,  ...,  0.9048,  0.2116, -0.6232]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>),)\n",
      "RobertaLayer forward inputs hidden_states: ftorch.Size([1, 8, 768])\n",
      "RobertaLayer forward inputs attention_mask: ftorch.Size([1, 1, 1, 8])\n",
      "RobertaLayer forward outputs: f(tensor([[[ 0.0340,  0.0365, -0.0324,  ...,  0.0364,  0.0444,  0.0399],\n",
      "         [-0.0319, -0.2987,  0.1366,  ...,  0.2968, -0.0466,  0.0558],\n",
      "         [ 0.1674,  0.1452, -0.1071,  ..., -0.6148,  0.0630,  0.2647],\n",
      "         ...,\n",
      "         [-0.0549,  0.2798,  0.5075,  ...,  0.2898, -0.3573,  0.4559],\n",
      "         [-0.6432,  0.3929,  0.1931,  ...,  0.9578, -0.9716,  0.0961],\n",
      "         [-0.2538,  0.1770,  0.1049,  ...,  0.8630,  0.0331, -0.4717]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>),)\n",
      "RobertaLayer forward inputs hidden_states: ftorch.Size([1, 8, 768])\n",
      "RobertaLayer forward inputs attention_mask: ftorch.Size([1, 1, 1, 8])\n",
      "RobertaLayer forward outputs: f(tensor([[[ 0.0083,  0.0671, -0.0097,  ...,  0.0360, -0.0248, -0.0322],\n",
      "         [-0.2826, -0.1344,  0.1996,  ...,  0.2180, -0.1554, -0.0234],\n",
      "         [-0.2179,  0.3775,  0.0082,  ..., -0.6167,  0.0490,  0.1369],\n",
      "         ...,\n",
      "         [-0.3686,  0.1852,  0.2229,  ...,  0.2237, -0.4062,  0.3308],\n",
      "         [-0.7002,  0.3990,  0.1529,  ...,  0.8473, -0.8314,  0.1829],\n",
      "         [ 0.2306,  0.0456,  0.2049,  ...,  0.2226,  0.1912, -0.1266]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>),)\n",
      "RobertaLayer forward inputs hidden_states: ftorch.Size([1, 8, 768])\n",
      "RobertaLayer forward inputs attention_mask: ftorch.Size([1, 1, 1, 8])\n",
      "RobertaLayer forward outputs: f(tensor([[[-4.2592e-04,  1.0514e-01,  1.2406e-02,  ..., -2.2557e-03,\n",
      "           2.3898e-02,  5.1967e-02],\n",
      "         [-3.0742e-02, -3.1318e-01,  2.7403e-01,  ...,  2.3672e-01,\n",
      "          -3.4039e-01, -2.0186e-01],\n",
      "         [ 6.5379e-03,  2.6569e-01, -1.9255e-01,  ..., -5.8500e-01,\n",
      "           2.0868e-01,  3.7995e-02],\n",
      "         ...,\n",
      "         [-2.6698e-01,  3.9142e-01,  4.0423e-01,  ...,  5.6909e-02,\n",
      "          -1.3024e-01,  6.6182e-02],\n",
      "         [-5.0145e-01,  6.0304e-01,  2.7315e-01,  ...,  4.9411e-01,\n",
      "          -6.3645e-01, -1.2867e-01],\n",
      "         [ 5.7506e-02, -3.5413e-02,  6.2207e-02,  ...,  1.1474e-01,\n",
      "          -2.0134e-02,  1.1450e-02]]], grad_fn=<NativeLayerNormBackward0>),)\n",
      "RobertaLayer forward inputs hidden_states: ftorch.Size([1, 8, 768])\n",
      "RobertaLayer forward inputs attention_mask: ftorch.Size([1, 1, 1, 8])\n",
      "RobertaLayer forward outputs: f(tensor([[[ 8.4833e-02,  7.3515e-02,  1.1009e-01,  ...,  2.2378e-02,\n",
      "          -1.5416e-02, -3.2401e-02],\n",
      "         [ 3.9331e-03,  1.6287e-02,  6.0952e-03,  ...,  1.0761e-01,\n",
      "          -3.1077e-01, -4.8785e-01],\n",
      "         [ 5.1321e-02,  2.9365e-01, -4.0065e-01,  ..., -6.3729e-01,\n",
      "           1.0847e-01, -1.9332e-01],\n",
      "         ...,\n",
      "         [ 4.7223e-02,  2.7136e-01, -2.1665e-02,  ...,  9.2003e-02,\n",
      "          -6.2781e-02, -1.1455e-01],\n",
      "         [-3.6399e-02,  7.2803e-01, -3.7197e-02,  ...,  4.8349e-01,\n",
      "          -3.3453e-01, -3.2218e-02],\n",
      "         [ 3.7163e-02,  5.1937e-03,  9.4846e-02,  ...,  3.4672e-04,\n",
      "          -5.7668e-02, -2.0398e-02]]], grad_fn=<NativeLayerNormBackward0>),)\n",
      "RobertaLayer forward inputs hidden_states: ftorch.Size([1, 8, 768])\n",
      "RobertaLayer forward inputs attention_mask: ftorch.Size([1, 1, 1, 8])\n",
      "RobertaLayer forward outputs: f(tensor([[[-0.0108,  0.0844,  0.0627,  ...,  0.1251, -0.0342,  0.0138],\n",
      "         [-0.0350, -0.0764, -0.0515,  ...,  0.1240, -0.1203, -0.3517],\n",
      "         [-0.0422,  0.0683,  0.1219,  ..., -0.7114, -0.0059, -0.2766],\n",
      "         ...,\n",
      "         [ 0.3014,  0.3367,  0.1607,  ...,  0.0682, -0.1288, -0.0862],\n",
      "         [ 0.3334,  0.6304, -0.0408,  ...,  0.4874, -0.3719, -0.2044],\n",
      "         [ 0.0127,  0.0071,  0.0651,  ...,  0.0178,  0.0074,  0.0289]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>),)\n",
      "RobertaLayer forward inputs hidden_states: ftorch.Size([1, 8, 768])\n",
      "RobertaLayer forward inputs attention_mask: ftorch.Size([1, 1, 1, 8])\n",
      "RobertaLayer forward outputs: f(tensor([[[-1.0948e-01,  1.2757e-01,  1.0420e-02,  ...,  1.2072e-01,\n",
      "           2.2446e-02, -3.1604e-02],\n",
      "         [ 1.2611e-01,  6.0357e-02,  6.9243e-02,  ...,  4.1631e-01,\n",
      "          -4.3807e-01, -6.3101e-01],\n",
      "         [ 1.2711e-01,  2.0162e-01, -8.4233e-02,  ..., -3.2182e-01,\n",
      "           9.0117e-02, -5.6850e-01],\n",
      "         ...,\n",
      "         [ 2.1811e-01,  6.0163e-01,  1.9932e-01,  ...,  8.1858e-02,\n",
      "          -3.3706e-01, -4.1537e-01],\n",
      "         [ 2.8873e-01,  9.2758e-01,  3.6009e-02,  ...,  4.7342e-01,\n",
      "          -4.2169e-01, -2.1731e-01],\n",
      "         [ 8.2146e-04,  4.0399e-02,  2.6152e-02,  ...,  8.5935e-03,\n",
      "          -6.2801e-04, -9.9431e-03]]], grad_fn=<NativeLayerNormBackward0>),)\n",
      "RobertaLayer forward inputs hidden_states: ftorch.Size([1, 8, 768])\n",
      "RobertaLayer forward inputs attention_mask: ftorch.Size([1, 1, 1, 8])\n",
      "RobertaLayer forward outputs: f(tensor([[[-8.6045e-02,  4.1524e-02, -1.1626e-01,  ...,  1.2701e-01,\n",
      "           4.5975e-02, -2.7339e-02],\n",
      "         [-5.3701e-02,  2.9082e-02, -1.6427e-01,  ...,  4.7388e-01,\n",
      "          -5.6919e-01, -2.9555e-01],\n",
      "         [ 4.2962e-01,  1.6103e-01, -5.3886e-02,  ..., -2.1000e-01,\n",
      "           1.0286e-01, -6.6340e-04],\n",
      "         ...,\n",
      "         [ 1.8374e-01,  7.5133e-01,  2.6152e-01,  ...,  2.6391e-01,\n",
      "          -1.1111e-01, -1.8614e-01],\n",
      "         [ 1.1871e-01,  4.2570e-01, -1.0245e-01,  ...,  7.1059e-01,\n",
      "           9.6509e-02,  1.0726e-01],\n",
      "         [ 4.2120e-02,  7.7366e-02,  1.3772e-01,  ..., -1.3649e-02,\n",
      "           1.2830e-01, -8.7909e-02]]], grad_fn=<NativeLayerNormBackward0>),)\n",
      "RobertaLayer forward inputs hidden_states: ftorch.Size([1, 8, 768])\n",
      "RobertaLayer forward inputs attention_mask: ftorch.Size([1, 1, 1, 8])\n",
      "RobertaLayer forward outputs: f(tensor([[[-0.0024,  0.0181, -0.0749,  ...,  0.0820,  0.0768,  0.0144],\n",
      "         [ 0.4075, -0.0655, -0.4623,  ...,  0.2750, -0.1931, -0.4159],\n",
      "         [ 0.4321,  0.2134, -0.2134,  ..., -0.3003,  0.1591, -0.0137],\n",
      "         ...,\n",
      "         [-0.0240,  0.5531,  0.2500,  ...,  0.1706, -0.2725, -0.1013],\n",
      "         [ 0.0515,  0.2569,  0.0066,  ...,  0.6839, -0.0423,  0.1127],\n",
      "         [-0.0683,  0.0313,  0.0162,  ...,  0.0313,  0.0970, -0.0104]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>),)\n",
      "RobertaLayer forward inputs hidden_states: ftorch.Size([1, 8, 768])\n",
      "RobertaLayer forward inputs attention_mask: ftorch.Size([1, 1, 1, 8])\n",
      "RobertaLayer forward outputs: f(tensor([[[-0.0073, -0.0334,  0.0062,  ...,  0.0821, -0.0763,  0.0591],\n",
      "         [ 0.2282, -0.2033,  0.1107,  ...,  0.2199, -0.0334, -0.3296],\n",
      "         [ 0.3692,  0.2551, -0.0284,  ..., -0.1725,  0.1952, -0.1002],\n",
      "         ...,\n",
      "         [-0.0274,  0.5019,  0.1576,  ...,  0.2170, -0.0387, -0.0299],\n",
      "         [ 0.2610,  0.0049,  0.1318,  ...,  0.5754, -0.2224,  0.0975],\n",
      "         [-0.0439, -0.0199,  0.0482,  ...,  0.0145, -0.0143,  0.0243]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>),)\n",
      "RobertaLayer forward inputs hidden_states: ftorch.Size([1, 8, 768])\n",
      "RobertaLayer forward inputs attention_mask: ftorch.Size([1, 1, 1, 8])\n",
      "RobertaLayer forward outputs: f(tensor([[[-0.0478,  0.0886, -0.0098,  ..., -0.0544, -0.0672, -0.0039],\n",
      "         [-0.0712,  0.0150, -0.1299,  ...,  0.0638,  0.0296, -0.0860],\n",
      "         [ 0.0906,  0.1437,  0.0828,  ...,  0.0509, -0.0320, -0.0490],\n",
      "         ...,\n",
      "         [ 0.0853,  0.2155,  0.0849,  ..., -0.1150,  0.0330, -0.0790],\n",
      "         [ 0.1679,  0.1288,  0.0065,  ...,  0.0367, -0.0631,  0.0276],\n",
      "         [-0.0436,  0.0892, -0.0389,  ..., -0.0957, -0.0744, -0.0284]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>),)\n"
     ]
    }
   ],
   "source": [
    "model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45d89b44-1ee5-426a-ae93-f8b717e3fdea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 31414,     6,   127,  2335,    16, 11962,     2]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a4a475a-aa13-45bd-aaea-c3a4e2849b09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0478,  0.0886, -0.0098,  ..., -0.0544, -0.0672, -0.0039],\n",
       "         [-0.0712,  0.0150, -0.1299,  ...,  0.0638,  0.0296, -0.0860],\n",
       "         [ 0.0906,  0.1437,  0.0828,  ...,  0.0509, -0.0320, -0.0490],\n",
       "         ...,\n",
       "         [ 0.0853,  0.2155,  0.0849,  ..., -0.1150,  0.0330, -0.0790],\n",
       "         [ 0.1679,  0.1288,  0.0065,  ...,  0.0367, -0.0631,  0.0276],\n",
       "         [-0.0436,  0.0892, -0.0389,  ..., -0.0957, -0.0744, -0.0284]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-3.2083e-03, -2.1940e-01, -2.1088e-01, -7.6721e-02,  1.2052e-01,\n",
       "          2.0488e-01,  2.6070e-01, -8.4344e-02, -7.2521e-02, -1.7025e-01,\n",
       "          2.1089e-01, -2.1008e-02, -8.2023e-02,  1.0177e-01, -1.4424e-01,\n",
       "          4.9201e-01,  2.1202e-01, -4.5743e-01,  3.5992e-02, -1.5410e-02,\n",
       "         -2.7218e-01,  8.2779e-02,  4.7007e-01,  3.3553e-01,  1.1576e-01,\n",
       "          6.0713e-02, -1.3375e-01, -1.2829e-02,  1.8398e-01,  2.2060e-01,\n",
       "          2.8606e-01,  6.4952e-02,  8.1128e-02,  2.3803e-01, -2.4037e-01,\n",
       "          6.3118e-02, -3.1076e-01,  2.3125e-02,  2.6080e-01, -1.8651e-01,\n",
       "         -7.9452e-02,  1.6443e-01,  2.1045e-01, -1.1840e-01, -1.1290e-01,\n",
       "          4.0572e-01,  2.5643e-01,  1.2402e-02, -1.4012e-01, -9.0373e-02,\n",
       "         -3.5431e-01,  3.3689e-01,  2.8484e-01,  1.9504e-01, -3.8898e-03,\n",
       "          5.9286e-02, -1.4538e-01,  2.5130e-01, -8.0821e-02, -9.2138e-02,\n",
       "         -1.1770e-01, -2.0288e-01, -1.3296e-02, -5.4601e-02,  2.9600e-02,\n",
       "         -1.4047e-01,  9.0024e-02, -1.4624e-01, -1.4190e-01,  5.4813e-02,\n",
       "         -8.5831e-02,  1.5300e-01,  1.6914e-01, -2.9596e-01, -2.9194e-01,\n",
       "          4.4366e-02, -5.8714e-01, -9.9223e-02,  2.9637e-01,  4.2659e-01,\n",
       "         -1.2049e-01,  1.8538e-01,  3.9121e-02,  2.1080e-01, -1.1053e-04,\n",
       "         -4.0333e-02, -3.0292e-02, -1.1374e-01,  1.9258e-01,  2.7085e-01,\n",
       "         -1.9872e-01, -3.7765e-01,  6.6629e-02,  1.1977e-02, -9.2962e-02,\n",
       "          2.1994e-02, -2.3806e-02, -9.6243e-02, -1.5413e-01, -1.6730e-01,\n",
       "          5.4596e-02, -2.6967e-01, -1.4077e-01,  2.6593e-01, -3.2843e-02,\n",
       "         -1.9712e-01, -7.7841e-03,  3.1161e-01,  7.4520e-02, -1.1691e-01,\n",
       "         -1.9002e-01,  4.2707e-01,  3.0891e-01,  1.9905e-04,  3.0902e-03,\n",
       "          1.7533e-01,  1.3256e-01, -2.9231e-01,  4.0937e-01, -3.1320e-01,\n",
       "         -1.7837e-02, -9.4238e-02,  1.1146e-01,  1.5994e-01, -2.2116e-01,\n",
       "          2.8568e-01,  1.3704e-01,  2.6840e-01,  1.8057e-01,  1.0697e-01,\n",
       "         -2.9501e-02,  1.4506e-01, -9.1473e-02,  1.3043e-01,  2.3266e-01,\n",
       "          1.1518e-01, -8.2935e-03, -3.2977e-01, -2.0575e-01,  2.6943e-01,\n",
       "          3.4101e-01,  1.5848e-01, -5.1063e-02,  1.8997e-01,  8.5725e-02,\n",
       "          2.2128e-01,  1.4372e-01, -4.1022e-01,  4.1918e-02,  3.5082e-01,\n",
       "          9.5941e-02,  1.5608e-01, -7.9715e-02, -2.8092e-01, -2.6411e-01,\n",
       "         -8.8306e-02,  5.0072e-02, -3.2875e-01, -1.3208e-01,  3.6530e-01,\n",
       "          2.1469e-02, -9.1301e-03, -1.4111e-01, -2.4637e-01, -2.3832e-02,\n",
       "         -1.2317e-01,  1.0649e-02,  1.1023e-01, -8.7270e-02, -4.0457e-01,\n",
       "         -8.6754e-02, -5.5748e-01, -1.0560e-01,  1.7113e-01, -3.2426e-01,\n",
       "          2.5368e-01, -2.7681e-01,  1.0262e-01,  4.0273e-01,  4.1042e-02,\n",
       "         -6.0417e-03, -1.8827e-01, -1.8964e-02,  9.2425e-02,  3.3184e-01,\n",
       "          2.4544e-01, -4.0065e-01,  1.1210e-01,  1.4380e-01,  2.5446e-01,\n",
       "          1.4272e-01, -5.5810e-02, -1.2864e-01,  1.5657e-01, -2.0099e-01,\n",
       "          1.7956e-01, -2.2109e-01,  1.8028e-01, -2.5524e-01, -2.1461e-01,\n",
       "          2.8902e-01, -3.9955e-01, -2.0351e-02,  8.7447e-02,  2.6895e-01,\n",
       "          1.2951e-02, -3.0363e-02, -7.7951e-02,  1.2212e-01,  1.9052e-01,\n",
       "          1.3431e-01, -3.8683e-01,  2.6582e-01, -2.7070e-02, -2.0792e-02,\n",
       "         -2.9300e-02,  1.6273e-01,  2.4574e-01,  9.6820e-02, -3.8570e-01,\n",
       "         -1.4093e-01,  1.1557e-01,  2.8804e-01, -2.2367e-01,  1.6416e-01,\n",
       "         -2.8154e-01, -3.9087e-01, -1.5021e-01,  2.1086e-01,  2.3105e-01,\n",
       "          1.6857e-01, -2.7287e-01,  1.6306e-01, -9.6196e-02, -4.2966e-01,\n",
       "         -3.6699e-01, -1.0738e-01,  2.5007e-01,  1.7285e-01,  1.8841e-01,\n",
       "          2.3728e-01,  3.9772e-02,  1.2035e-01,  1.4498e-01,  1.5580e-01,\n",
       "         -1.5020e-01,  1.9035e-01, -3.5627e-01, -5.8517e-02, -2.6624e-01,\n",
       "         -1.9203e-01, -1.9466e-01,  3.9838e-01, -2.3064e-01,  2.3329e-01,\n",
       "          3.8728e-01, -3.0407e-01, -1.1507e-01,  1.4859e-01,  9.1981e-02,\n",
       "          9.4698e-02, -1.1759e-01,  1.9530e-01,  1.4820e-01, -1.0942e-01,\n",
       "          2.5250e-01,  2.3120e-03,  2.5350e-01,  1.6860e-01,  8.7136e-02,\n",
       "          1.3513e-01,  1.2709e-01, -1.4862e-01,  6.0751e-02,  9.3345e-03,\n",
       "         -1.5235e-02, -2.3177e-01, -1.5835e-01,  2.3159e-01, -5.2097e-02,\n",
       "          2.3692e-02, -1.7081e-01, -1.1216e-01,  2.9000e-02,  4.0367e-01,\n",
       "         -3.5810e-01,  2.5118e-01,  7.6397e-02,  1.5772e-01, -2.4908e-01,\n",
       "         -2.2593e-01,  8.8132e-02,  1.7857e-01, -4.1115e-01,  1.0124e-02,\n",
       "          1.7034e-01,  9.3064e-02,  2.0605e-01,  2.6011e-01,  8.8563e-03,\n",
       "         -1.1599e-01,  4.9098e-01, -1.6107e-01, -1.0854e-01,  2.5703e-01,\n",
       "         -2.7082e-01, -2.7938e-01,  2.4896e-01, -2.8771e-02,  2.9932e-01,\n",
       "          1.2622e-01,  5.4038e-02,  7.4917e-02, -6.0018e-01,  6.1034e-02,\n",
       "         -4.5302e-01,  9.6109e-03,  2.2219e-02, -8.2516e-02, -1.9849e-01,\n",
       "          1.4898e-01,  2.9653e-01, -2.5667e-01, -2.8793e-02,  1.8863e-01,\n",
       "          6.9257e-02, -1.2353e-01,  4.7548e-01, -1.0399e-02,  2.1192e-01,\n",
       "         -5.5704e-02,  2.7205e-01, -2.1447e-01,  2.6845e-01, -2.7371e-01,\n",
       "         -7.9793e-02,  1.9678e-02,  7.9384e-02,  6.9016e-02, -6.2468e-02,\n",
       "         -3.3971e-01,  2.3516e-01, -7.9372e-03, -5.4151e-02, -3.6996e-02,\n",
       "          1.0210e-01,  8.3840e-04,  4.8332e-02,  5.8442e-02,  3.1268e-01,\n",
       "          2.2963e-01, -1.5988e-02, -3.7113e-01, -2.7166e-02, -8.3582e-02,\n",
       "          3.8697e-02,  4.8938e-02, -1.5783e-02,  4.3969e-01, -8.1168e-02,\n",
       "          3.5309e-03, -1.4284e-01,  2.6100e-01,  2.1125e-01,  1.2627e-01,\n",
       "          1.2150e-01,  5.6126e-02,  1.2790e-01, -5.3467e-02, -7.7731e-03,\n",
       "         -1.5671e-01, -2.3348e-01, -2.7533e-01,  2.0882e-01, -2.3905e-01,\n",
       "         -1.6444e-01,  1.6090e-01,  2.1307e-01, -1.5447e-01,  1.4451e-01,\n",
       "          3.0499e-01,  1.0366e-01, -1.4900e-01,  2.6629e-01, -1.0861e-01,\n",
       "          9.7824e-02,  3.1754e-01, -1.5510e-02,  1.7810e-01,  4.9086e-01,\n",
       "          2.1100e-01, -3.6921e-01, -3.9460e-02, -2.1250e-01, -2.9532e-03,\n",
       "          2.3747e-01, -1.4730e-01,  2.0255e-01,  3.8513e-01,  3.0637e-01,\n",
       "          4.5606e-01,  8.6698e-03, -1.3313e-01,  8.3101e-02,  2.1972e-01,\n",
       "          3.5209e-02, -1.5206e-01, -1.8271e-01,  2.5397e-01,  5.9857e-02,\n",
       "         -1.4193e-01, -3.8307e-02, -1.4553e-01,  4.7165e-02, -1.3325e-01,\n",
       "         -3.9433e-01,  4.2436e-02,  1.9429e-01, -4.8001e-01,  8.4262e-02,\n",
       "         -2.7683e-01,  3.6440e-02, -2.3495e-01,  2.1043e-01, -2.1483e-01,\n",
       "         -1.1695e-01,  3.9920e-01, -8.2238e-02,  5.3366e-02, -1.8199e-01,\n",
       "         -1.4275e-01,  2.0019e-02,  1.1507e-02, -3.6021e-02, -2.6190e-02,\n",
       "          3.3418e-01, -1.3056e-01,  3.2988e-02,  2.2499e-02,  2.0725e-01,\n",
       "         -3.8817e-02,  1.9868e-01,  2.0599e-02, -1.3215e-01, -3.7478e-01,\n",
       "          1.3769e-01, -1.9972e-01, -4.2155e-01, -3.7575e-01,  3.5446e-01,\n",
       "         -1.2160e-01, -2.4805e-01, -2.1167e-01, -2.5744e-01,  7.9576e-02,\n",
       "          1.7727e-01,  4.6106e-01, -3.8363e-01, -8.9918e-02,  4.6798e-01,\n",
       "         -6.3311e-02, -1.8311e-01,  2.9651e-01,  1.8038e-01, -3.3584e-01,\n",
       "          3.3358e-01,  2.5996e-01, -4.4390e-02,  2.1432e-02,  5.1450e-01,\n",
       "          1.2765e-01,  2.0913e-01, -2.3102e-01,  4.4982e-01, -2.2196e-01,\n",
       "          3.2071e-01, -1.4369e-01, -2.1302e-01, -2.2435e-01, -5.3516e-03,\n",
       "          3.3233e-01,  1.8082e-01, -4.2011e-01, -1.1286e-01,  4.2059e-02,\n",
       "          3.3800e-01, -3.8471e-01, -9.5194e-02,  1.4146e-02, -3.5604e-01,\n",
       "          1.0313e-01,  9.5098e-02,  2.2574e-01, -3.7865e-01,  3.6247e-03,\n",
       "          4.0502e-01, -3.0943e-01,  1.2550e-01,  3.0371e-01,  8.1005e-02,\n",
       "          3.3761e-01, -4.3828e-02, -3.5592e-03,  4.5577e-02, -2.2371e-01,\n",
       "         -3.7315e-02,  1.3860e-01,  5.5346e-01,  1.4818e-01, -3.8170e-01,\n",
       "          8.5319e-02,  2.4381e-01, -1.7700e-01,  3.0341e-01, -8.3246e-02,\n",
       "         -5.3749e-02,  2.6875e-01, -4.7404e-02,  1.4566e-01, -8.5108e-02,\n",
       "         -2.3477e-01, -3.0237e-01,  3.7206e-01, -2.1425e-01, -1.1400e-01,\n",
       "         -1.5839e-01, -1.1716e-01, -1.4141e-01,  3.6058e-02, -3.9509e-01,\n",
       "          3.3778e-01,  1.2326e-01, -2.1119e-01, -9.7114e-02, -9.1938e-02,\n",
       "         -1.6368e-01, -2.2534e-01, -2.6148e-01,  4.4172e-01, -1.5848e-01,\n",
       "         -4.5032e-01,  2.5770e-01,  2.4804e-02,  3.4088e-01,  3.5758e-02,\n",
       "          1.0179e-01, -4.3535e-02,  1.3006e-01,  1.0202e-01, -1.2319e-01,\n",
       "          2.6838e-01,  5.6350e-02, -5.5955e-01, -1.2166e-01, -2.0871e-01,\n",
       "          7.0198e-02,  1.9283e-01, -3.4027e-01,  1.8359e-02,  3.0427e-02,\n",
       "          1.4175e-01,  2.0520e-02, -1.1092e-01, -7.2368e-02,  4.0556e-01,\n",
       "          2.3859e-01,  2.8483e-01,  9.0411e-02,  2.4168e-01, -1.5583e-02,\n",
       "         -3.3556e-01,  3.5719e-02,  8.4716e-02, -1.9951e-01,  4.2450e-01,\n",
       "         -1.0325e-01, -3.9120e-01, -6.8124e-02,  3.9697e-01,  1.1035e-01,\n",
       "         -2.6056e-02, -4.7428e-02,  2.0151e-01,  1.6093e-01, -1.3766e-01,\n",
       "          1.9095e-01, -4.2122e-02, -1.4172e-01, -1.1627e-01,  8.0168e-02,\n",
       "         -2.1798e-01,  4.4219e-02, -1.4472e-01, -9.5638e-03, -2.0081e-01,\n",
       "          7.9597e-03, -1.9085e-01,  2.5598e-01, -3.3713e-01,  1.1407e-01,\n",
       "          7.0708e-02,  2.9457e-01, -3.4398e-01, -1.6568e-01, -6.8814e-02,\n",
       "          1.5692e-01,  2.6861e-01,  3.4846e-01,  2.1363e-02,  1.2183e-02,\n",
       "         -1.5440e-01, -2.5803e-01,  7.1844e-02, -1.9390e-01,  1.3984e-01,\n",
       "          7.0863e-02,  2.6211e-01, -3.0336e-01, -1.8847e-01,  2.2263e-01,\n",
       "         -1.0745e-01, -1.5428e-01,  4.0873e-01,  2.4101e-01,  2.1457e-01,\n",
       "          2.5258e-02,  2.4745e-01,  3.0405e-02, -1.7431e-01, -1.2609e-01,\n",
       "         -2.5028e-01,  7.9450e-02, -7.4164e-02, -5.3013e-02, -6.4149e-02,\n",
       "         -1.0449e-01, -2.0651e-01, -1.6621e-01,  1.3898e-01,  1.4613e-01,\n",
       "          3.6762e-02, -5.0540e-02, -2.2787e-02, -2.7661e-01,  2.9619e-01,\n",
       "          2.0211e-02,  7.4367e-02, -6.5166e-02,  2.9048e-02, -1.4591e-01,\n",
       "          2.4087e-01,  2.1074e-01,  9.0950e-02, -1.9921e-01, -4.8711e-02,\n",
       "         -2.8255e-01, -3.6132e-01,  6.1972e-02,  1.3865e-01,  1.0557e-01,\n",
       "         -6.8828e-02, -2.8253e-01, -1.0248e-02, -1.4579e-01,  1.7499e-01,\n",
       "          1.5391e-02, -1.5322e-01, -8.2757e-02, -5.3386e-02, -5.5117e-02,\n",
       "          7.9734e-02, -2.1771e-01, -1.8813e-01, -1.1873e-01, -6.5332e-02,\n",
       "         -7.3576e-02,  3.5305e-01, -4.6619e-02,  2.8208e-01, -1.4859e-01,\n",
       "          1.0735e-03, -1.6637e-01,  1.1365e-01, -6.2794e-02,  6.2111e-02,\n",
       "          2.7882e-01, -4.4071e-01, -1.6083e-01, -2.9695e-03, -2.1326e-01,\n",
       "         -1.6536e-01, -6.1523e-02, -3.7208e-02,  2.3069e-01, -3.4058e-01,\n",
       "          2.3236e-01, -1.2023e-01,  1.9255e-01, -7.2060e-02, -2.5519e-01,\n",
       "         -1.5923e-01,  1.6881e-02,  2.4711e-01, -3.4725e-01, -2.3262e-01,\n",
       "         -2.7701e-01, -1.0317e-01, -9.3272e-02, -2.6039e-01,  4.2547e-01,\n",
       "         -1.0798e-01, -7.4447e-02,  1.9067e-02,  4.4333e-01,  2.0667e-01,\n",
       "          1.4671e-01,  2.1272e-01, -1.5556e-02,  2.9830e-02,  1.1363e-01,\n",
       "         -4.5824e-01,  2.2959e-01, -2.3105e-01, -1.1901e-01, -2.5509e-03,\n",
       "          8.7938e-02, -2.8578e-02,  1.0000e-02, -1.3353e-01, -1.1202e-01,\n",
       "          2.1694e-01, -3.6830e-01, -3.1860e-02,  2.5587e-01,  1.5419e-01,\n",
       "         -2.5011e-01,  4.0122e-02,  1.1360e-01,  3.7682e-01,  1.0048e-01,\n",
       "         -2.2502e-01,  1.2753e-01, -3.4334e-01, -4.1958e-02, -1.9138e-01,\n",
       "         -2.9233e-01,  1.5540e-01, -6.1172e-02,  7.5229e-02, -8.8815e-02,\n",
       "         -2.9441e-01,  2.1590e-01, -5.9937e-02, -7.0518e-02,  4.1725e-01,\n",
       "          3.5135e-02, -1.3039e-01,  1.5322e-01,  6.3149e-03,  7.2841e-03,\n",
       "         -1.0152e-01,  2.6193e-01,  2.1660e-01, -2.7404e-01,  1.4459e-01,\n",
       "         -1.3454e-01, -3.1701e-02, -1.1210e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
